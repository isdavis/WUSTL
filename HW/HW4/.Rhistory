abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda", ylab="Male Coefficient")
abline(h=0.94083, col="red")
par(mfrow=c(3,1))
plot(elastic.lambda,elastic.result, xlab="Lambda", ylab="Male Coefficient", xlim=c(0,1))
abline(h=0.94083, col="red")
plot(ridge.lambda,ridge.result,xlab="Lambda", ylab="Male Coefficient", xlim=c(0,1))
abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda", ylab="Male Coefficient", xlim=c(0,1))
abline(h=0.94083, col="red")
par(mfrow=c(3,1))
plot(elastic.lambda,elastic.result, xlab="Lambda", ylab="Male Coefficient", xlim=c(0,1.5))
abline(h=0.94083, col="red")
plot(ridge.lambda,ridge.result,xlab="Lambda", ylab="Male Coefficient", xlim=c(0,1.5))
abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda", ylab="Male Coefficient", xlim=c(0,1.5))
abline(h=0.94083, col="red")
pdf("lambda_male.pdf")
par(mfrow=c(3,1))
plot(elastic.lambda,elastic.result, xlab="Lambda", ylab="Male Coefficient", ylim=c(0,1.5))
# add line from lm() result
abline(h=0.94083, col="red")
plot(ridge.lambda,ridge.result,xlab="Lambda", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
dev.off()
par(mfrow=c(3,1))
plot(elastic.lambda,elastic.result, xlab="Lambda", ylab="Male Coefficient", ylim=c(0,1.5))
# add line from lm() result
abline(h=0.94083, col="red")
plot(ridge.lambda,ridge.result,xlab="Lambda", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
pdf("lambda_male.pdf")
par(mfrow=c(3,1))
plot(elastic.lambda,elastic.result, xlab="Lambda (in Elastic-Net Regression)", ylab="Male Coefficient", ylim=c(0,1.5))
# add line from lm() result
abline(h=0.94083, col="red")
plot(ridge.lambda,ridge.result,xlab="Lambda (in Ridge Regression)", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda (in Lasso Regression)", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
dev.off()
par(mfrow=c(3,1))
plot(elastic.lambda,elastic.result, xlab="Lambda (in Elastic-Net Regression)", ylab="Male Coefficient", ylim=c(0,1.5))
# add line from lm() result
abline(h=0.94083, col="red")
plot(ridge.lambda,ridge.result,xlab="Lambda (in Ridge Regression)", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
plot(lasso.lambda,lasso.result,xlab="Lambda (in Lasso Regression)", ylab="Male Coefficient", ylim=c(0,1.5))
abline(h=0.94083, col="red")
index <- sample(seq_len(nrow(covariates)), size = 20)
train <- covariates[index,]
x.train <- covariates[-index,]
x.text <- covariates[index,]
y.train <- alcohol[-index,]
y.text <- alcohol[index,]
y.train <- alcohol[-index]
y.text <- alcohol[index]
index <- sample(seq_len(nrow(covariates)), size = 20)
x.train <- covariates[-index,]
x.test <- covariates[index,]
y.train <- alcohol[-index]
y.test <- alcohol[index]
rm(list=ls())
#load data
setwd('~/Dropbox/2017_Classes/Text-Analysis/WUSTL/HW/HW4/')
# read data
load("~/Dropbox/2017_Classes/Text-Analysis/WUSTL/StudentDrinking.RData")
covariates <- X # covariates
rm(X)
# check data
head(covariates) # where is codebook? url does not working
head(alcohol) # what is the unit? consumption?
# i) linear regression
lm.mod <- lm(alcohol ~ covariates)
summary(lm.mod)
# being male, going out with friends more, and skipping classes have positive influence on alcohol consumption
# studying more time, considering family time have negative influence on alchol consumption
# ii) lasso
library(glmnet)
glmnet.mod <- cv.glmnet(y=alcohol, x=covariates, alpha=1,family="gaussian", type.measure="mse")
print(glmnet.mod)
plot(glmnet.mod)
glmnet.mod$lambda.min
# 0.05751821
glmnet.mod$lambda.1se
# 0.232202
coef(glmnet.mod, s = glmnet.mod$lambda.min)
# iii) ridge
ridge.mod <- cv.glmnet(y=alcohol, x=covariates, alpha=0,family="gaussian", type.measure="mse")
print(ridge.mod)
plot(ridge.mod)
ridge.mod$lambda.min
# 0.3784314
ridge.mod$lambda.1se
# 4.251011
coef(ridge.mod, s = ridge.mod$lambda.min)
# iv) elastic-net regression
elastic.mod <- cv.glmnet(y=alcohol, x=covariates, alpha=0.5,family="gaussian", type.measure="mse")
print(elastic.mod)
plot(elastic.mod$lambda,elastic.mod$cvm)
plot(elastic.mod)
elastic.mod$lambda.min
# 0.1048169
elastic.mod$lambda.1se
# 0.464404
coef(elastic.mod, s = elastic.mod$lambda.min)
# alpha: alpha is the elasticnet mixing parameter, which ranges from 0 to 1. The penalty is 1 for lasso penalty and 0 for ridge penalty.
# for elastic-net regression, alpha=0.5, which indicates the balance between Lasso and Ridge approaches.
# v)
# Lambda is the tuning parameter which determents the amount of shrinkage
elastic.result <- as.matrix(coef(elastic.mod, s = elastic.mod$lambda))[3,]
elastic.lambda <- elastic.mod$lambda
ridge.result <- as.matrix(coef(ridge.mod, s = ridge.mod$lambda))[3,]
ridge.lambda <- ridge.mod$lambda
lasso.result <- as.matrix(coef(glmnet.mod, s = glmnet.mod$lambda))[3,]
lasso.lambda <- glmnet.mod$lambda
# i)
index <- sample(seq_len(nrow(covariates)), size = 20)
x.train <- covariates[-index,]
x.test <- covariates[index,]
y.train <- alcohol[-index]
y.test <- alcohol[index]
index <-c(1:20)
x.train <- covariates[-index,]
x.test <- covariates[index,]
y.train <- alcohol[-index]
y.test <- alcohol[index]
elastic.mod <- cv.glmnet(y=y.train, x=x.train, alpha=0.5,family="gaussian", type.measure="mse")
print(elastic.mod)
lasso.mod <- cv.glmnet(y=y.train, x=x.train, alpha=1,family="gaussian", type.measure="mse")
print(lasso.mod)
ridge.mod <- cv.glmnet(y=y.train, x=x.train, alpha=0,family="gaussian", type.measure="mse")
print(ridge.mod)
elastic.mod <- cv.glmnet(y=y.train, x=x.train, alpha=0.5,family="gaussian", type.measure="mse")
print(elastic.mod)
index <-c(1:20)
x.train <- covariates[-index,]
x.valid <- covariates[index,]
y.train <- alcohol[-index]
y.valid <- alcohol[index]
index <- sample(seq_len(nrow(subdat)), size = 10)
index <- sample(seq_len(nrow(x.train)), size = 10)
train <- data.matrix(x.train[index,])
test <- data.matrix(x.train[-index,])
training.y <- alcohol[index,]
training.y <- alcohol[index]
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix(x.train[index,])
training.y <- alcohol[-index]
testing.y <- alcohol[index]
index <- rep(NA, 10)
index <- ls()
index <- ls()
index[1] <- sample(seq_len(nrow(x.train)), size = 10)
sample(seq_len(nrow(x.train)), size = 10)
sample(seq_len(nrow(x.train)), size = 10)
sample(seq_len(nrow(x.train)), size = 10)
index[[1]] <- sample(seq_len(nrow(x.train)), size = 10)
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix(x.train[index,])
training.y <- alcohol[-index]
testing.y <- alcohol[index]
lm.mod <- lm(training.y ~ training.x)
lmd.pred <- predict(lm.mod, newdata=testing.x)
}
index <-c(1:20)
x.train <- covariates[-index,]
x.valid <- covariates[index,]
y.train <- alcohol[-index]
y.valid <- alcohol[index]
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix(x.train[index,])
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
lmd.pred <- predict(lm.mod, newdata=testing.x)
}
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix(x.train[index,])
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
lmd.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
}
training.x
lm.mod
testing.x <- as.data.frame((x.train[index,]))
testing.x
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
lmd.pred <- predict(lm.mod, newdata=testing.x)
}
summary(lm.mod)
View(testing.x)
View(testing.x)
x.train[-index,]
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- as.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
lm.pred <- predict(lm.mod, newdata=testing.x)
}
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
lm.pred <- predict(lm.mod, newdata=testing.x)
}
training.x
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ data.frame(training.x))
lm.pred <- predict(lm.mod, newdata=testing.x)
}
lm.mod <- lm(training.y ~ training.x)
summary(lm.mod)
rownames(lm.mod$coef) <- rownames(training.x)
summary(lm.mod)
rownames(training.x)
training.x
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
rownames(lm.mod$coef) <- colnames(training.x)
lm.pred <- predict(lm.mod, newdata=testing.x)
}
rownames(lm.mod$coef) <- colnames(training.x)
colnames(training.x)
rownames(lm.mod$coef) <- c("intercept", colnames(training.x))
c("intercept", colnames(training.x))
rownames(lm.mod$coef)
lm.mod$coef
rownames(lm.mod)
lm.mod$coef
colnames(lm.mod$coef)
testing.y <- y.train[index]
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=testing.x)
}
names(lm.mod$coef)
c("intercept", colnames(training.x))
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=testing.x)
}
View(testing.x)
lasso.mod <- cv.glmnet(y=y.train, x=x.train, alpha=1,family="gaussian", type.measure="mse")
print(lasso.mod)
lasso.pred <- predict(lasso.mod, newx=testing.x, s = lasso.mod$lambda.min)
View(testing.x)
View(training.x)
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- as.data.frame((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=testing.x)
}
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=testing.x)
}
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
lm.mod <- lm(training.y ~ training.x)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
}
lm.mod <- lm(training.y ~ training.x)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
summary(lm.mod)
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
lm.mod <- lm(training.y ~ training.x, dat=trainingDat)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
}
head(trainingDat)
lm.mod <- lm(training.y ~ trainingDat[,c(2:21)], data=trainingDat)
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
lm.mod <- lm(training.y ~ trainingDat[,c(2:21)], data=trainingDat)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
}
trainingDat[,c(2:21)]
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
lm.mod <- lm(training.y ~ ., data=trainingDat)
names(lm.mod$coef) <- c("intercept", colnames(training.x))
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
}
summary(lm.mod)
# linear regression
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
lm.mod <- lm(training.y ~ ., data=trainingDat)
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
}
lm.pred
lasso.mod <- cv.glmnet(y=y.train, x=trainingDat[,c(2:21)], alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
View(trainingDat)
lasso.mod <- cv.glmnet(y=trainingDat$y.train, x=trainingDat[,c(2:21)], alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
trainingDat[,c(2:21)]
lasso.mod <- cv.glmnet(y=trainingDat$y.train, x=trainingDat[,c(2:21)], alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
lasso.mod <- cv.glmnet(y=trainingDat$y.train, x=training.x, alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
# linear regression
lm.mod <- lm(training.y ~ ., data=trainingDat)
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
# Lasso
lasso.mod <- cv.glmnet(y=training.y, x=training.x, alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
# print(lasso.mod)
lasso.pred <- predict(lasso.mod, newx=testing.x, s = lasso.mod$lambda.min)
}
lasso.pred
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
# linear regression
lm.mod <- lm(training.y ~ ., data=trainingDat)
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
# Lasso
lasso.mod <- cv.glmnet(y=training.y, x=training.x, alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
# print(lasso.mod)
lasso.pred <- predict(lasso.mod, newx=testing.x, s = lasso.mod$lambda.min)
# Ridge
ridge.mod <- cv.glmnet(y=training.y, x=training.x, alpha=0,family="gaussian", type.measure="mse")
ridge.pred <- predict(ridge.mod, newx=testing.x, s = ridge.mod$lambda.min)
# Elastic-Net
elastic.mod <- cv.glmnet(y=training.y, x=training.x, alpha=0.5,family="gaussian", type.measure="mse")
elastic.pred <- predict(elastic.mod, newx=testing.x, s = elastic.mod$lambda.min)
}
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
# linear regression
lm.mod <- lm(training.y ~ ., data=trainingDat)
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
# Lasso
lasso.mod <- cv.glmnet(y=training.y, x=training.x, alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
# print(lasso.mod)
lasso.pred <- predict(lasso.mod, newx=testing.x, s = lasso.mod$lambda.min)
# Ridge
ridge.mod <- cv.glmnet(y=training.y, x=training.x, alpha=0,family="gaussian", type.measure="mse")
ridge.pred <- predict(ridge.mod, newx=testing.x, s = ridge.mod$lambda.min)
# Elastic-Net
elastic.mod <- cv.glmnet(y=training.y, x=training.x, alpha=0.5,family="gaussian", type.measure="mse")
elastic.pred <- predict(elastic.mod, newx=testing.x, s = elastic.mod$lambda.min)
# random forest
forest.mod <- randomForest(y=training.y, x=training.x)
# get predicted value
forest.pred<- predict(forest.mod, newdata=testing.x)
}
library(randomForest)
for(i in 1:10){
index <- sample(seq_len(nrow(x.train)), size = 10)
training.x <- data.matrix(x.train[-index,])
testing.x <- data.matrix((x.train[index,]))
training.y <- y.train[-index]
testing.y <- y.train[index]
trainingDat <- as.data.frame(cbind(training.y, training.x))
# linear regression
lm.mod <- lm(training.y ~ ., data=trainingDat)
lm.pred <- predict(lm.mod, newdata=as.data.frame(testing.x))
# Lasso
lasso.mod <- cv.glmnet(y=training.y, x=training.x, alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
# print(lasso.mod)
lasso.pred <- predict(lasso.mod, newx=testing.x, s = lasso.mod$lambda.min)
# Ridge
ridge.mod <- cv.glmnet(y=training.y, x=training.x, alpha=0,family="gaussian", type.measure="mse")
ridge.pred <- predict(ridge.mod, newx=testing.x, s = ridge.mod$lambda.min)
# Elastic-Net
elastic.mod <- cv.glmnet(y=training.y, x=training.x, alpha=0.5,family="gaussian", type.measure="mse")
elastic.pred <- predict(elastic.mod, newx=testing.x, s = elastic.mod$lambda.min)
# random forest
forest.mod <- randomForest(y=training.y, x=training.x)
# get predicted value
forest.pred<- predict(forest.mod, newdata=testing.x)
}
cbind(lm.pred, lasso.pred, ridge.pred,
elastic.pred, forest.pred) -1)$coefficients
superleaner <- lm(y.train ~ cbind(lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)-1)
cbind(lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)
superleaner <- lm(y.train ~ cbind(lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)-1)
elastic.pred
prediction <- cbind(lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)
prediction <- cbind(lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)
prediction <- data.matrix(prediction)
superleaner <- lm(y.train ~ prediction-1)
prediction
prediction <- cbind(y.train, lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)
prediction <- as.data.frame(prediction)
y.train
testing.y
prediction <- cbind(testing.y, lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)
prediction <- as.data.frame(prediction)
superleaner <- lm(testing.y ~ .-1, data=prediction)
prediction <- as.data.frame(prediction)
prediction <- cbind(testing.y, lm.pred, lasso.pred, ridge.pred,elastic.pred, forest.pred)
prediction <- as.data.frame(prediction)
head(prediction)
names(prediction) <- c("y", "lm", "lasso", "ridge", "elast", "forest")
head(prediction)
superleaner <- lm(y ~ .-1, data=prediction)
summary(superlearner)
summary(superleaner)
superleaner$coefficients
weights <- superleaner$coefficients
superweights <- superleaner$coefficients
wholetraining <- as.data.frame(cbind(x.train, y.train))
lm.train <- lm(y.train ~ ., data=wholetraining)
lm.train.pred <- predict(lm.train, newdata=as.data.frame(x.valid))
lasso.train <- cv.glmnet(y=y.train, x=x.train, alpha=1,family="gaussian", type.measure="mse") # cv.glmnet does N-fold crossvalidation with N=10 by default
lasso.train.pred <- predict(lasso.train, newx=x.valid, s = lasso.train$lambda.min)
ridge.train <- cv.glmnet(y=y.train, x=x.train, alpha=0,family="gaussian", type.measure="mse")
ridge.train.pred <- predict(ridge.train, newx=x.valid, s = ridge.train$lambda.min)
elastic.train <- cv.glmnet(y=y.train, x=x.train, alpha=0.5,family="gaussian", type.measure="mse")
elastic.train.pred <- predict(elastic.train, newx=x.valid, s = elastic.train$lambda.min)
forest.train <- randomForest(y=y.train, x=x.train)
forest.train.pred<- predict(forest.train, newdata=x.valid)
forest.train.pred
